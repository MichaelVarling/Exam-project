{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "from glob import glob\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class_path = [r\"C:\\Users\\crisp\\OneDrive\\Skrivebord\\eXAM\\bikes\\bikes\", r'C:\\Users\\crisp\\OneDrive\\Skrivebord\\eXAM\\bikes\\non-bikes']\n",
    "class_labels = [\"bikes\", \"non-bikes\"]\n",
    "val_class_path = [r\"C:\\Users\\crisp\\OneDrive\\Skrivebord\\eXAM\\validation\", r'C:\\Users\\crisp\\OneDrive\\Skrivebord\\eXAM\\val_non_bikes']\n",
    "val_images, val_labels = [], []\n",
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies = [], []\n",
    "batch_size = 64\n",
    "num_classes = 2\n",
    "num_epochs = 120\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image):\n",
    "    if image.shape[0] == 1:\n",
    "        image = image.repeat(3, 1, 1)\n",
    "    elif image.shape[0] == 4:\n",
    "        image = image[:3]\n",
    "    image = image.float() / 255\n",
    "    image = torchvision.transforms.functional.resize(image, [64, 64], antialias=True)\n",
    "    return image\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomRotation(85),  # Rotate images by up to 10 degrees\n",
    "    transforms.Resize((64, 64)),    # Resize images (modify as per your requirement)\n",
    "    transforms.ToTensor(),          # Convert images to Tensor\n",
    "    # Add any other transformations like normalization if required\n",
    "])\n",
    "\n",
    "images, labels = [], []\n",
    "extensions = ['jpeg', 'jpg', 'png']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2027 images in class 'bikes'.\n",
      "Found 1864 images in class 'non-bikes'.\n"
     ]
    }
   ],
   "source": [
    "for i, label in enumerate(class_labels):\n",
    "    filenames = []\n",
    "    for ext in extensions:\n",
    "        filenames.extend(glob(os.path.join(class_path[i], f'*.{ext}')))\n",
    "    \n",
    "    print(f\"Found {len(filenames)} images in class '{label}'.\")\n",
    "    for file in filenames:\n",
    "        try:\n",
    "            image = torchvision.io.read_image(file)\n",
    "            image = preprocess(image)\n",
    "            images.append(image)\n",
    "            labels.append(i)  # Label as 0 or 1\n",
    "            val_images.append(image)\n",
    "            val_labels.append(i)  # Label as 0 or 1\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load image {file}: {e}\")\n",
    "\n",
    "        # except Exception as e:\n",
    "        #     print(f\"Failed to load image {file}: {e}\")\n",
    "if not val_images:\n",
    "    print(\"No validation images were loaded. Please check the dataset and file paths.\")\n",
    "else:\n",
    "    val_images_tensor = torch.stack(val_images).float()\n",
    "    val_labels_tensor = torch.tensor(val_labels).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "images_tensor = torch.stack(images).float()\n",
    "labels_tensor = torch.tensor(labels).float()\n",
    "val_images_tensor = torch.stack(val_images).float()\n",
    "val_labels_tensor = torch.tensor(val_labels).float()\n",
    "\n",
    "# Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "train_data = TensorDataset(images_tensor, labels_tensor)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_data = TensorDataset(val_images_tensor, val_labels_tensor)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network\n",
    "net = nn.Sequential(\n",
    "    nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2),\n",
    "    nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(64 * 16 * 16, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(128, 1)\n",
    ").to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "rotation_transform = transforms.RandomRotation(85)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m acc\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mnum_epochs\u001b[49m):\n\u001b[0;32m     14\u001b[0m     epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     15\u001b[0m     epoch_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_epochs' is not defined"
     ]
    }
   ],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch\n",
    "    \"\"\"\n",
    "    # round predictions to the closest integer (0 or 1)\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()  # convert into float for division\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "# Training loop\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for x, y in train_loader:\n",
    "        # Apply rotation transform to each image in the batch\n",
    "        x = torch.stack([rotation_transform(img) for img in x])\n",
    "\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        # ... rest of your training loop ...\n",
    "    # Training phase\n",
    "    net.train()\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = net(x)\n",
    "\n",
    "        loss = loss_function(out.squeeze(), y)\n",
    "        acc = binary_accuracy(out.squeeze(), y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    # Validation phase\n",
    "    net.eval()\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for x_val, y_val in val_loader:\n",
    "            x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "            val_out = net(x_val)\n",
    "            val_loss += loss_function(val_out.squeeze(), y_val).item()\n",
    "            val_acc += binary_accuracy(val_out.squeeze(), y_val).item()\n",
    "\n",
    "    # Calculating average loss and accuracy\n",
    "    train_loss = epoch_loss / len(train_loader)\n",
    "    train_acc = epoch_acc / len(train_loader)\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc /= len(val_loader)\n",
    "\n",
    "\n",
    "    # Save metrics\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    # Printing the results for the current epoch\n",
    "    \n",
    "# Plotting\n",
    "\n",
    "    # Printing the results for the current epoch\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}')\n",
    "\n",
    "# Save the model after training\n",
    "torch.save(net.state_dict(), 'bike_Cycle_model.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
