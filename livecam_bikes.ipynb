{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the model architecture\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2),\n",
    "    nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(64 * 16 * 16, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(128, 1)\n",
    ")\n",
    "\n",
    "# Load the pre-trained model weights\n",
    "model.load_state_dict(torch.load('Cycle_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "def processframe(frame):\n",
    "    # Resize frame to the expected input size of the model\n",
    "    # The model expects an input of size (64, 16, 16) after the conv and pool layers\n",
    "    # You need to adjust this to match the input size of your model before the first conv layer\n",
    "    resized_frame = cv2.resize(frame, (64, 64))  # Example, resize to 64x64\n",
    "\n",
    "    # Convert color to RGB (if your model was trained on RGB images)\n",
    "    rgb_frame = cv2.cvtColor(resized_frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Normalize the frame\n",
    "    # Assuming your model was trained with images having pixel values [0,1] or [-1,1]\n",
    "    # Adjust these values based on how your model was trained\n",
    "    normalized_frame = rgb_frame / 255.0\n",
    "\n",
    "    return normalized_frame\n",
    "\n",
    "\n",
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "...\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Pre-process the frame\n",
    "    processed_frame = processframe(frame)\n",
    "\n",
    "    # Convert frame to tensor\n",
    "    frame_tensor = torch.from_numpy(processed_frame).permute(2, 0, 1).unsqueeze(0).float()\n",
    "\n",
    "    # Detect bikes\n",
    "    with torch.no_grad():\n",
    "        output = model(frame_tensor)\n",
    "        probability = torch.sigmoid(output).item()\n",
    "\n",
    "    # Threshold for detection\n",
    "    threshold = 0.5  # You may adjust this threshold as needed\n",
    "\n",
    "    # Check if the model detects a bike\n",
    "    if probability > threshold:\n",
    "        # Indicate bike detection on the frame\n",
    "        cv2.putText(frame, \"Bike Detected\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Bike Detection', frame)\n",
    "\n",
    "    # Break the loop\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything is done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
